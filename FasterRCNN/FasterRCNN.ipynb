{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Libaray"
   ],
   "metadata": {
    "id": "CE_iZSCfzTN-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guN02B3MCHM1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image,ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import xml.etree.ElementTree as ET\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "id": "wBArUXOAIYrZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "id": "WCNRFRIkg8rG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preperation"
   ],
   "metadata": {
    "id": "2TA8j0mv-EE0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Annotations directory path\n",
    "ann_directory = 'archive/annotations'"
   ],
   "metadata": {
    "id": "__eQwzaYhqCF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Image directory path\n",
    "img_directory = 'archive/images'"
   ],
   "metadata": {
    "id": "CUOeefY5hzA-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encoded_labels(lst_labels):\n",
    "    \"\"\"Encodes label classes from string to integers.\n",
    "\n",
    "        Labels are encoded accordingly:\n",
    "            - with_mask => 1\n",
    "            - mask_weared_incorrect => 2\n",
    "            - without_mask => 0\n",
    "\n",
    "            Args:\n",
    "              lst_labels:\n",
    "                A list with classes in string format (e.g. ['with_mask', 'mask_weared_incorrect'...]).\n",
    "\n",
    "            Returns:\n",
    "              encoded:\n",
    "                A list with integers that represent each class.\n",
    "            \"\"\"\n",
    "\n",
    "    encoded=[]\n",
    "    for label in lst_labels:\n",
    "        if label == \"with_mask\":\n",
    "            code = 1\n",
    "        elif label == \"mask_weared_incorrect\":\n",
    "            code = 2\n",
    "        else:\n",
    "            code = 0\n",
    "        encoded.append(code)\n",
    "    return encoded"
   ],
   "metadata": {
    "id": "SfpuhpcZj2Jj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyDataset():\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, ann_dir, img_dir, transform=None, mode='train'):\n",
    "\n",
    "        # Image directories\n",
    "        self.ann_dir = ann_dir\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create dataframe to hold info\n",
    "        self.data = pd.DataFrame(columns=['Filename', 'BoundingBoxes', 'Labels', 'Area', 'N_Objects'])\n",
    "\n",
    "        # Append rows with image filename and respective bounding boxes to the df\n",
    "        for file in enumerate(os.listdir(img_dir)):\n",
    "            # fileï¼š(2, 'maksssksksss102.png')\n",
    "\n",
    "            # Find image annotation file\n",
    "            ann_file_path = os.path.join(ann_dir, file[1][:-4]) + '.xml'\n",
    "  \n",
    "            # Read XML file and return bounding boxes and class attributes\n",
    "            objects = self.read_XML_classf(ann_file_path)\n",
    "\n",
    "            # Create list of labels in an image\n",
    "            list_labels = encoded_labels(objects[0]['labels'])\n",
    "\n",
    "            # Create list of bounding boxes in an image\n",
    "            list_bb = []\n",
    "            list_area = []\n",
    "            n_obj = len(objects[0]['objects'])\n",
    "            for i in objects[0]['objects']:\n",
    "                list = [i['xmin'], i['ymin'], i['xmax'], i['ymax']]\n",
    "                list_bb.append(list)\n",
    "                list_area.append((i['xmax'] - i['xmin']) * (i['ymax'] - i['ymin']))\n",
    "\n",
    "            # Create dataframe object with row containing [(Image file name),(Bounding Box List)]\n",
    "            df = pd.DataFrame([[file[1], list_bb, list_labels, list_area, n_obj]],\n",
    "                              columns=['Filename', 'BoundingBoxes', 'Labels', 'Area', 'N_Objects'])\n",
    "            # self.data = self.data.append(df)\n",
    "            self.data = self.data.append(df)\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.data = self.data[:680]\n",
    "        elif mode == 'test':\n",
    "            self.data = self.data[680:850]\n",
    "\n",
    "        # Number of images in dataset\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "        # Get the length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Image file path\n",
    "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n",
    "\n",
    "        # Open image file and tranform to tensor\n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        # Get bounding box coordinates\n",
    "        bbox = torch.tensor(self.data.iloc[idx, 1])\n",
    "\n",
    "        # Get labels\n",
    "        labels = torch.tensor(self.data.iloc[idx, 2])\n",
    "\n",
    "        # Get bounding box areas\n",
    "        area = torch.tensor(self.data.iloc[idx, 3])\n",
    "\n",
    "        # If any, apply tranformations to image and bounding box mask\n",
    "        if self.transform:\n",
    "            # Convert PIL image to numpy array\n",
    "            img = np.array(img)\n",
    "            # Apply transformations\n",
    "            transformed = self.transform(image=img, bboxes=bbox)\n",
    "            # Convert numpy array to PIL Image\n",
    "            img = Image.fromarray(transformed['image'])\n",
    "            # Get transformed bb\n",
    "            bbox = torch.tensor(transformed['bboxes'])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        num_objs = self.data.iloc[idx, 4]\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Transform img to tensor\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "\n",
    "        # Build Targer dict\n",
    "        target= {\"boxes\": bbox, \"labels\": labels, \"image_id\": torch.tensor([idx]), \"area\": area, \"iscrowd\": iscrowd}\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    # XML reader -> returns dictionary with image bounding boxes sizes\n",
    "    def read_XML_classf(self, ann_file_path):\n",
    "        bboxes = [{\n",
    "            'file': ann_file_path,\n",
    "            'labels': [],\n",
    "            'objects': []\n",
    "        }]\n",
    "\n",
    "        # Reading XML file objects and print Bounding Boxes\n",
    "        tree = ET.parse(ann_file_path)\n",
    "        root = tree.getroot()\n",
    "        objects = root.findall('object')\n",
    "\n",
    "        for obj in objects:\n",
    "            # label\n",
    "            label = obj.find('name').text\n",
    "            bboxes[0]['labels'].append(label)\n",
    "\n",
    "            # bbox dimensions\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            bboxes[0]['objects'].append({'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})\n",
    "        return bboxes"
   ],
   "metadata": {
    "id": "R-Bmkiyufou8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    # Collate function for Dataloader\n",
    "    return tuple(zip(*batch))"
   ],
   "metadata": {
    "id": "hHCel6_FiJek",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#  Create Data Pipeline\n",
    "# Training Data\n",
    "dataset_train = MyDataset(ann_directory,img_directory, mode = 'train')\n",
    "print(len(dataset_train))\n",
    "loader_train = DataLoader(dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "# Testing Data\n",
    "dataset_test = MyDataset(ann_directory,img_directory, mode = 'test')\n",
    "print(len(dataset_test))\n",
    "loader_test = DataLoader(dataset_test, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "id": "0BvxuYgPgXXO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model DEF"
   ],
   "metadata": {
    "id": "lUfd69rY-L8w",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    print(\"in_features:\", in_features)\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "Vdgpyln9ITEB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = get_model_instance_segmentation(3)"
   ],
   "metadata": {
    "id": "45QxQFw2IWVZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "id": "-hi6nMNdyy4C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model\n"
   ],
   "metadata": {
    "id": "OowjO5KZ98cf",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set Hyper-parameters\n",
    "# Network params\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Optimizers\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# Learning Rate, lr decreases to half every 2 epochs \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "# Number of epochs to perform\n",
    "epoch= 4"
   ],
   "metadata": {
    "id": "iGDlfLV-tJmL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, loader, optimizer, scheduler, epochs, device):\n",
    "  \"\"\" \n",
    "    Inputs:\n",
    "      - model\n",
    "      - loader: Dataloader PyTorch object with training data\n",
    "      - optimizer\n",
    "      - scheduler\n",
    "      - epochs\n",
    "      - device\n",
    "\n",
    "    Returns:\n",
    "      - model\n",
    "      - loss_list: list with mean loss per epoch. Epoch 1 is in idex 0.\n",
    "      - lr_list: list with learning rate per epoch. Epoch 1 is in idex 0.\n",
    "    \"\"\"\n",
    "  # Create a loss list to keep epoch average loss\n",
    "  loss_list = []\n",
    "  lr_list = []\n",
    "  # Epochs\n",
    "  for epoch in range(epochs):\n",
    "      print('Starting epoch...... {}/{} '.format(epoch + 1, epochs))\n",
    "      iteration = 0\n",
    "      loss_sub_list = []\n",
    "      start = time.time()\n",
    "      for images, targets in loader:\n",
    "          # Agregate images in batch loader\n",
    "          images = list(image.to(device) for image in images)\n",
    "\n",
    "          # Agregate targets in batch loader\n",
    "          targets = [{key: val.to(device) for key, val in target.items()} for target in targets]\n",
    "\n",
    "          # Sets model to train mode (just a flag)\n",
    "          model.train()\n",
    "\n",
    "          # Output of model returns loss and detections\n",
    "          optimizer.zero_grad()\n",
    "          output = model(images, targets)\n",
    "\n",
    "          # Calculate Cost\n",
    "          losses = sum(loss for loss in output.values())\n",
    "          loss_value = losses.item()\n",
    "          loss_sub_list.append(loss_value)\n",
    "          # print('')\n",
    "\n",
    "          # Update optimizer and learning rate\n",
    "          losses.backward()\n",
    "          optimizer.step()\n",
    "          iteration += 1\n",
    "          # print('Iteration: {:d} --> Loss: {:.3f}'.format(iteration, loss_value))\n",
    "          \n",
    "      end = time.time()\n",
    "      # update scheduler\n",
    "      current_lr = optimizer.param_groups[0]['lr']\n",
    "      lr_list.append(current_lr)\n",
    "      scheduler.step()\n",
    "      # print the loss of epoch\n",
    "      epoch_loss = np.mean(loss_sub_list)\n",
    "      loss_list.append(epoch_loss)\n",
    "      print('Epoch loss: {:.3f} , time used: ({:.1f}s)'.format(epoch_loss, end - start))\n",
    "      \n",
    "  return model, loss_list, lr_list"
   ],
   "metadata": {
    "id": "V-cMDhy_syP0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model,loss_history,learning_rate_history = train_model(model, loader_train, optimizer, lr_scheduler,epoch,device)"
   ],
   "metadata": {
    "id": "d8sHCE7irrlF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "PATH = \"FasterRCNN/model_epoch_4.pth\""
   ],
   "metadata": {
    "id": "umjSP8QHDV9g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), PATH)"
   ],
   "metadata": {
    "id": "TcILIkWWiQWw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# drawing loss and learning_rat\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "ax1.plot(range(len(learning_rate_history)), learning_rate_history, color='tab:blue')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Learning Rate')\n",
    "ax2.plot(range(len(loss_history)), loss_history, color='tab:red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "wUICpR-1pEhf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation DEF"
   ],
   "metadata": {
    "id": "XahECdsnaSGc",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def apply_nms(orig_prediction, iou_thresh):\n",
    "    \"\"\"\n",
    "    Applies non max supression and eliminates low score bounding boxes.\n",
    "\n",
    "      Args:\n",
    "        orig_prediction: the model output. A dictionary containing element scores and boxes.\n",
    "        iou_thresh: Intersection over Union threshold. Every bbox prediction with an IoU greater than this value\n",
    "                      gets deleted in NMS.\n",
    "\n",
    "      Returns:\n",
    "        final_prediction: Resulting prediction\n",
    "    \"\"\"\n",
    "\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "\n",
    "    # Keep indices from nms\n",
    "    final_prediction = orig_prediction\n",
    "\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "\n",
    "    return final_prediction"
   ],
   "metadata": {
    "id": "67gJECj8P2I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def IOU(box1, box2):\n",
    "    '''\n",
    "    Intersection over Union - IoU\n",
    "    *------------\n",
    "    |   (x2min,y2min)\n",
    "    |   *----------\n",
    "    |   | ######| |\n",
    "    ----|------* (x1max,y1max)\n",
    "        |         |\n",
    "        ----------\n",
    "\n",
    "    Args:\n",
    "        box1: [xmin,ymin,xmax,ymax]\n",
    "        box2: [xmin,ymin,xmax,ymax]\n",
    "\n",
    "    Returns:\n",
    "        iou -> value of intersection over union of the 2 boxes\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Compute coordinates of intersection\n",
    "    xmin_inter = max(box1[0], box2[0])\n",
    "    ymin_inter = max(box1[1], box2[1])\n",
    "    xmax_inter = min(box1[2], box2[2])\n",
    "    ymax_inter = min(box1[3], box2[3])\n",
    "\n",
    "    # calculate area of intersection rectangle\n",
    "    inter_area = max(0, xmax_inter - xmin_inter + 1) * max(0, ymax_inter - ymin_inter + 1) # FIXME why plus one?\n",
    " \n",
    "    # calculate boxes areas\n",
    "    area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    " \n",
    "    # compute IoU\n",
    "    iou = inter_area / float(area1 + area2 - inter_area)\n",
    "    assert iou >= 0\n",
    "    return iou"
   ],
   "metadata": {
    "id": "dlnDOVYjNMlh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_AP(ground_truth, predictions, iou_thresh, n_classes):\n",
    "    \"\"\"\n",
    "    Calculates Average Precision across all classes.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
    "        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
    "        iou_thresh: IoU to which a prediction compared to a ground-truth is considered right.\n",
    "        n_classes: number of existent classes\n",
    "\n",
    "    Returns:\n",
    "        Average precision for the specified threshold.\n",
    "    \"\"\"\n",
    "    # Initialize lists\n",
    "    APs = []\n",
    "    \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    TP_all = []\n",
    "    FP_all = []\n",
    "   \n",
    "    epsilon = 1e-6\n",
    "\n",
    "    # AP is computed for each class\n",
    "    for c in range(n_classes):\n",
    "        class_gt = []\n",
    "        class_predictions = []\n",
    "        # Find gt and predictions of the class\n",
    "        for gt in ground_truth:\n",
    "            if gt[4] == c:\n",
    "                class_gt.append(gt)\n",
    "        for predict in predictions:\n",
    "            if predict[4] == c:\n",
    "                class_predictions.append(predict)\n",
    "\n",
    "        # Create dict with array of zeros for bb in each image\n",
    "        gt_amount_bb = Counter([gt[1] for gt in class_gt])\n",
    "        for key, val in gt_amount_bb.items():\n",
    "            gt_amount_bb[key] = np.zeros(val)\n",
    "\n",
    "        # Sort class predictions by their score\n",
    "        class_predictions = sorted(class_predictions, key=lambda x: x[5], reverse=True)\n",
    "\n",
    "        # Create arrays for Positives (True and False)\n",
    "        TP = np.zeros(len(class_predictions))\n",
    "        FP = np.zeros(len(class_predictions))\n",
    "        # Number of true boxes\n",
    "        truth = len(class_gt)\n",
    "\n",
    "        # Initializing aux variables\n",
    "        \n",
    "\n",
    "        # Iterate over predictions in each image and compare with ground truth\n",
    "        for predict_idx, prediction in enumerate(class_predictions):\n",
    "            # Filter prediction image ground truths\n",
    "            image_gt = [obj for obj in class_gt if obj[1] == prediction[1]]\n",
    "\n",
    "            # Initializing aux variables\n",
    "            best_iou = -1\n",
    "            best_gt_iou_idx = -1\n",
    "\n",
    "            # Iterate through image ground truths and calculate IoUs\n",
    "            for gt_idx, gt in enumerate(image_gt):\n",
    "                iou = IOU(prediction[3], gt[3])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_iou_idx = gt_idx\n",
    "\n",
    "            # If the best IoU is greater that thresh than an TP prediction has been found\n",
    "            if best_iou > iou_thresh and best_gt_iou_idx > -1:\n",
    "                # Check if gt box was already covered\n",
    "                if  gt_amount_bb[prediction[1]][best_gt_iou_idx] == 0:\n",
    "                    gt_amount_bb[prediction[1]][best_gt_iou_idx] = 1  # set as covered\n",
    "                    TP[predict_idx] = 1  # Count as true positive\n",
    "                else:\n",
    "                    FP[predict_idx] = 1\n",
    "            else:\n",
    "                FP[predict_idx] = 1\n",
    "\n",
    "        # Calculate recall and precision\n",
    "        TP_cumsum = np.cumsum(TP)\n",
    "        FP_cumsum = np.cumsum(FP)\n",
    "        recall = np.append([0], TP_cumsum / (truth + epsilon))\n",
    "        precision = np.append([1], np.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon)))\n",
    "\n",
    "        # Calculate the area precision/recall and add to list\n",
    "        APs.append(np.trapz(precision, recall))\n",
    "    \n",
    "    return sum(APs)/len(APs) # average of class precisions\n",
    "\n",
    "\n",
    "def compute_mAP(ground_truth, predictions, n_classes):\n",
    "    \"\"\"\n",
    "    Calls AP computation for different levels of IoUs, [0.5:.05:0.95].\n",
    "\n",
    "    Args:\n",
    "        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
    "        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n",
    "        n_classes: number of existent classes.\n",
    "\n",
    "    Returns:\n",
    "        mAp and list with APs for each IoU threshold.\n",
    "    \"\"\"\n",
    "    APs = [compute_AP(ground_truth, predictions, iou_thresh, n_classes) for iou_thresh in np.arange(0.5, 1.0, 0.05)]\n",
    "    return np.mean(APs), APs"
   ],
   "metadata": {
    "id": "rtUA7xiaPey3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, sequences=1):\n",
    "    \"\"\"\n",
    "    Evaluates model mAP for IoU range of [0.5:.05:0.95].\n",
    "\n",
    "    Args:\n",
    "        model: -\n",
    "        data_loader: -\n",
    "        device: -\n",
    "        sequences: the number of sequences of images to pass, if any\n",
    "\n",
    "    Returns:\n",
    "        mAP and AP list for each IoU threshold in range [0.5:.05:0.95]\n",
    "    \"\"\"\n",
    "\n",
    "    # Set evaluation mode flag\n",
    "    model.eval()\n",
    "    # Create list with all object detection -> [set, frame, obj, [xmin,ymin,xmax,ymax], label, score]\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "\n",
    "    # Gather all targets and outputs on test set\n",
    "    for image, targets in data_loader:\n",
    "        image = [img.to(device) for img in image]\n",
    "        outputs = model(image)\n",
    "        for idx in range(len(outputs)):\n",
    "            outputs[idx] = apply_nms(outputs[idx], iou_thresh=0.5)\n",
    "\n",
    "        # create list for targets and outputs to pass to compute_mAP()\n",
    "        # lists have the following structure:  [sequence, frame, obj_idx, [xmin, ymin, xmax, ymax], label, score]\n",
    "        for s in range(sequences):\n",
    "            obj_gt = 0\n",
    "            obj_target = 0\n",
    "            for out, target in zip(outputs, targets):\n",
    "\n",
    "                for i in range(len(target['boxes'])):\n",
    "                    ground_truth.append([s, target['image_id'].detach().cpu().numpy()[0], obj_target,\n",
    "                                         target['boxes'].detach().cpu().numpy()[i],\n",
    "                                         target['labels'].detach().cpu().numpy()[i], 1])\n",
    "                    obj_target += 1\n",
    "\n",
    "                for j in range(len(out['boxes'])):\n",
    "                    predictions.append([s, target['image_id'].detach().cpu().numpy()[0], obj_gt,\n",
    "                                        out['boxes'].detach().cpu().numpy()[j],\n",
    "                                        out['labels'].detach().cpu().numpy()[j],\n",
    "                                        out['scores'].detach().cpu().numpy()[j]])\n",
    "                    obj_gt += 1\n",
    "\n",
    "    # mAP, AP, recall,precision = compute_mAP(ground_truth, predictions, 3)\n",
    "    mAP, AP = compute_mAP(ground_truth, predictions, 3)\n",
    "    print(\"mAP:{:.3f}\".format(mAP))\n",
    "    for ap_metric, iou in zip(AP, np.arange(0.5, 1, 0.05)):\n",
    "        print(\"\\tAP at IoU level [{:.2f}]: {:.3f}\".format(iou, ap_metric))\n",
    "\n",
    "    return mAP, AP"
   ],
   "metadata": {
    "id": "4Jl6ENfQzvhE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model loading and evaluating"
   ],
   "metadata": {
    "id": "oL06fgoLaNyh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "PATH = 'FasterRCNN/model_epoch_25.pth'"
   ],
   "metadata": {
    "id": "sS99IqH2D3jE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_eval = model.load_state_dict(torch.load(PATH))"
   ],
   "metadata": {
    "id": "fchxP6T6DOix",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "mAP, AP= evaluate(model, loader_test, device, sequences=1)"
   ],
   "metadata": {
    "id": "Lxkxmfo8M77g",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "metadata": {
    "id": "R2HtDaiu_-j4",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def decode_labels(lst_labels):\n",
    "    \"\"\"\n",
    "    Decode label classes from integers to strings.\n",
    "    Labels are encoded accordingly:\n",
    "        - background => 0\n",
    "        - with_mask => 1\n",
    "        - mask_weared_incorrect => 2\n",
    "        - without_mask => 3\n",
    "\n",
    "    Args:\n",
    "      lst_labels:\n",
    "        A list with classes in integer format (e.g. [1, 2, ...]).\n",
    "\n",
    "    Returns:\n",
    "        A list with strings that represent each class.\n",
    "    \"\"\"\n",
    "\n",
    "    labels=[]\n",
    "    for code in lst_labels:\n",
    "        if code == 1:\n",
    "            label = \"with_mask\"\n",
    "        elif code == 2:\n",
    "            label = \"mask_weared_incorrect\"\n",
    "        elif code == 3:\n",
    "            label = \"without_mask\"\n",
    "        else:\n",
    "            label = 'background'\n",
    "        labels.append(label)\n",
    "    return labels"
   ],
   "metadata": {
    "id": "mwsV3GzW8bwb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def draw_bounding_boxes(img_tensor, target=None, prediction=None):\n",
    "    \"\"\"Draws bounding boxes in given images. Displays them\n",
    "\n",
    "        Inputs:\n",
    "          img:\n",
    "            Image in tensor format.\n",
    "          target:\n",
    "            target dictionary containing bboxes list wit format -> [xmin, ymin, xmax, ymax]\n",
    "\n",
    "        Returns:\n",
    "          None\n",
    "        \"\"\"\n",
    "\n",
    "    img = torchvision.transforms.ToPILImage()(img_tensor)\n",
    "\n",
    "    # fetching the dimensions\n",
    "    wid, hgt = img.size\n",
    "    print(str(wid) + \"x\" + str(hgt))\n",
    "\n",
    "    # Img to draw in\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    if target:\n",
    "        target_bboxes = target['boxes'].numpy().tolist()\n",
    "        target_labels = decode_labels(target['labels'].numpy())\n",
    "\n",
    "        for i in range(len(target_bboxes)):\n",
    "            # Create Rectangle patches and add the patches to the axes\n",
    "            draw.rectangle(target_bboxes[i], fill=None, outline='green', width=2)\n",
    "            draw.text(target_bboxes[i][:2], target_labels[i], fill='green', font=None, anchor=None, spacing=4,\n",
    "                      align='left', direction=None, features=None, language=None, stroke_width=0, stroke_fill=None,\n",
    "                      embedded_color=False)\n",
    "\n",
    "    if prediction:\n",
    "        prediction_bboxes = prediction['boxes'].detach().cpu().numpy().tolist()\n",
    "        prediction_labels = decode_labels(prediction['labels'].detach().cpu().numpy())\n",
    "        for i in range(len(prediction_bboxes)):\n",
    "            # Create Rectangle patches and add the patches to the axes\n",
    "            draw.rectangle(prediction_bboxes[i], fill=None, outline='red', width=2)\n",
    "            draw.text(prediction_bboxes[i][:2], prediction_labels[i], fill='red', font=None, anchor=None, spacing=4,\n",
    "                      align='left', direction=None, features=None, language=None, stroke_width=0, stroke_fill=None,\n",
    "                      embedded_color=False)\n",
    "\n",
    "    display(img)"
   ],
   "metadata": {
    "id": "ZZ7kdABt7wzW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_low_score_bb(orig_prediction, score_thresh):\n",
    "    \"\"\"\n",
    "    Eliminates low score bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        orig_prediction: the model output. A dictionary containing element scores and boxes.\n",
    "        score_thresh: Boxes with a lower confidence score than this value get deleted\n",
    "\n",
    "    Returns:\n",
    "        final_prediction: Resulting prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove low confidence scores according to given threshold\n",
    "    index_list_scores = []\n",
    "    scores = orig_prediction['scores'].detach().cpu().numpy()\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] > score_thresh:\n",
    "            index_list_scores.append(i)\n",
    "    keep = torch.tensor(index_list_scores)\n",
    "\n",
    "    # Keep indices from high score bb\n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "\n",
    "    return final_prediction"
   ],
   "metadata": {
    "id": "3rzBQXMC9eIF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Make prediction on random image\n",
    "img, target = dataset_test[60]\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])[0]\n",
    "\n",
    "# Non max suppression to reduce the number of bounding boxes\n",
    "nms_prediction = apply_nms(prediction, iou_thresh=0.5)\n",
    "\n",
    "# Draw bounding boxes\n",
    "draw_bounding_boxes(img.detach().cpu(), target=target, prediction=nms_prediction)"
   ],
   "metadata": {
    "id": "GEw9crBX7RhE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}